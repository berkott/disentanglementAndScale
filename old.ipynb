{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WC0tPemnVwf"
      },
      "source": [
        "# Basic CLIP Example\n",
        "A lot of this code is copied from my repo: https://github.com/berkott/cvInterp\n",
        "\n",
        "Also, upload bunny.zip from the paper\n",
        "\n",
        "CLIP repo + download instructions: https://github.com/openai/CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TPL9Kx4TnVwk"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "/home/berk/anaconda3/envs/python38/lib/python3.8/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/berk/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/optimize/_highs/_highs_wrapper.cpython-38-x86_64-linux-gnu.so)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/home/berk/code/disentanglementAndScale/old.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcrv01/home/berk/code/disentanglementAndScale/old.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcrv01/home/berk/code/disentanglementAndScale/old.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcrv01/home/berk/code/disentanglementAndScale/old.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcrv01/home/berk/code/disentanglementAndScale/old.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression, Lasso, LassoCV\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcrv01/home/berk/code/disentanglementAndScale/old.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearSVC\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/stats/__init__.py:467\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    466\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 467\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    468\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    469\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/stats/_stats_py.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     49\u001b[0m                                    siegelslopes)\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/stats/distributions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m (comb, chndtr, entr, xlogy, ive)\n\u001b[1;32m     22\u001b[0m \u001b[39m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# estimation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[1;32m     26\u001b[0m \u001b[39m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m integrate\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/__init__.py:211\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(name):\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m submodules:\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m _importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscipy.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    212\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/optimize/__init__.py:413\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_nnls\u001b[39;00m \u001b[39mimport\u001b[39;00m nnls\n\u001b[1;32m    412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_basinhopping\u001b[39;00m \u001b[39mimport\u001b[39;00m basinhopping\n\u001b[0;32m--> 413\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linprog\u001b[39;00m \u001b[39mimport\u001b[39;00m linprog, linprog_verbose_callback\n\u001b[1;32m    414\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lsap\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_sum_assignment\n\u001b[1;32m    415\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_differentialevolution\u001b[39;00m \u001b[39mimport\u001b[39;00m differential_evolution\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/optimize/_linprog.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizeResult, OptimizeWarning\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linprog_highs\u001b[39;00m \u001b[39mimport\u001b[39;00m _linprog_highs\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linprog_ip\u001b[39;00m \u001b[39mimport\u001b[39;00m _linprog_ip\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linprog_simplex\u001b[39;00m \u001b[39mimport\u001b[39;00m _linprog_simplex\n",
            "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/optimize/_linprog_highs.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m _check_unknown_options, OptimizeWarning, OptimizeResult\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_highs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_highs_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m _highs_wrapper\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_highs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_highs_constants\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     CONST_I_INF,\n\u001b[1;32m     23\u001b[0m     CONST_INF,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     HIGHS_VAR_TYPE_CONTINUOUS,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csc_matrix, vstack, issparse\n",
            "\u001b[0;31mImportError\u001b[0m: /home/berk/anaconda3/envs/python38/lib/python3.8/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/berk/anaconda3/envs/python38/lib/python3.8/site-packages/scipy/optimize/_highs/_highs_wrapper.cpython-38-x86_64-linux-gnu.so)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "from six.moves import range\n",
        "# from sklearn import svm\n",
        "# import gin.tf\n",
        "import os\n",
        "import random\n",
        "import scipy.stats\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import mutual_info_score, roc_auc_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.ensemble.forest import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#TODO:\n",
        "\n",
        "#Get Bunny images\n",
        "#Get pipieline\n",
        "#Test SAP\n",
        "#TEst NIG\n",
        "\n",
        "#Get Geomancer\n",
        "#G\n",
        "\n",
        "\n",
        "#Do different VAE sizes (find different B-VAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_GXjQ2_nVwm"
      },
      "outputs": [],
      "source": [
        "clip.clip._MODELS = {\n",
        "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
        "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
        "    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "}\n",
        "\n",
        "MODELS_TO_USE = [ \"RN50\", \"RN101\", \"RN50x4\"]\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBH_w3mlnVwn"
      },
      "source": [
        "## Model Param Details\n",
        "I think we should just start by running the experiments on RN50, RN101, and RN50x4. These are 3 resnet architectures that are basically just scaled up to different sizes. Then if we have extra time, we cna test on everything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h0zV4XDTnVwo"
      },
      "outputs": [],
      "source": [
        "# Get model param details\n",
        "# for name in clip.clip._MODELS_TO_USE:\n",
        "#     m, _ = clip.load(name, device=device, jit=False)\n",
        "#     print(name)\n",
        "#     print(f\"Num Params: {sum(p.numel() for p in m.parameters())}\")\n",
        "#     print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97-VUhaWnVwp"
      },
      "outputs": [],
      "source": [
        "image_paths = os.listdir('./drive/MyDrive/images')\n",
        "images = []\n",
        "for path in image_paths[:1000]:\n",
        "  images.append(preprocess(Image.open(\"./drive/MyDrive/images/\" + path)).unsqueeze(0).to(device)) # TODO: Specify correct image here!!\n",
        "\n",
        "# T.ToPILImage()(image[0]).show()\n",
        "\n",
        "\n",
        "# text = clip.tokenize([\"bunny\"]).to(device)\n",
        "# image_encodings = []\n",
        "# for image in images:\n",
        "#   with torch.no_grad():\n",
        "#       image_encodings.append(model.encode_image(image))\n",
        "#       text_features = model.encode_text(text)\n",
        "\n",
        "#       # THIS IMAGE FEATURES VECTOR IS THE LATENT REPRESENTATION WE WANT!!!\n",
        "#       # print(image_features.shape)\n",
        "#       # print(image_features)\n",
        "      \n",
        "#       logits_per_image, logits_per_text = model(image, text)\n",
        "#       probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "#   print(\"Label probs:\", probs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Xgz1CkCBqg"
      },
      "source": [
        "# Mine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dSprites(path):\n",
        "    # part of the code is from:\n",
        "    # https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_reloading_example.ipynb\n",
        "    dataset_zip = np.load(\n",
        "        os.path.join(\n",
        "            path, \"dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
        "        allow_pickle=True)\n",
        "    imgs = dataset_zip['imgs']\n",
        "    latent_values = dataset_zip['latents_values']\n",
        "    #latents_classes = dataset_zip['latents_classes']\n",
        "    metadata = dataset_zip['metadata'][()]\n",
        "\n",
        "    imgs = imgs.reshape(737280, 64, 64, 1).astype(np.float)  # 0 ~ 1\n",
        "\n",
        "    latents_names = metadata[\"latents_names\"]\n",
        "    latents_sizes = metadata[\"latents_sizes\"]\n",
        "    latents_possible_values = metadata[\"latents_possible_values\"]\n",
        "    latents_bases = np.concatenate(\n",
        "        (latents_sizes[::-1].cumprod()[::-1][1:], np.array([1, ])))\n",
        "\n",
        "    def latent_to_index(latents):\n",
        "        return np.dot(latents, latents_bases).astype(int)\n",
        "\n",
        "    def sample_latent(size=1):\n",
        "        samples = np.zeros((size, latents_sizes.size))\n",
        "        for lat_i, lat_size in enumerate(latents_sizes):\n",
        "            samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
        "        return samples\n",
        "\n",
        "    metric_data_groups = []\n",
        "    L = 100\n",
        "    M = 500\n",
        "\n",
        "    for i in range(M):\n",
        "        fixed_latent_id = i % 5 + 1\n",
        "        latents_sampled = sample_latent(size=L)\n",
        "        latents_sampled[:, fixed_latent_id] = \\\n",
        "            np.random.randint(latents_sizes[fixed_latent_id], size=1)\n",
        "        # print(latents_sampled[0:10])\n",
        "        indices_sampled = latent_to_index(latents_sampled)\n",
        "        imgs_sampled = imgs[indices_sampled]\n",
        "        metric_data_groups.append(\n",
        "            {\"img\": imgs_sampled,\n",
        "             \"label\": fixed_latent_id - 1})\n",
        "\n",
        "    selected_ids = np.random.permutation(range(imgs.shape[0]))\n",
        "    selected_ids = selected_ids[0: imgs.shape[0] / 10]\n",
        "    metric_data_eval_std = imgs[selected_ids]\n",
        "\n",
        "    random_latent_ids = sample_latent(size=imgs.shape[0] / 10)\n",
        "    random_latent_ids = random_latent_ids.astype(np.int32)\n",
        "    random_ids = latent_to_index(random_latent_ids)\n",
        "    assert random_latent_ids.shape == (imgs.shape[0] / 10, 6)\n",
        "    random_imgs = imgs[random_ids]\n",
        "\n",
        "    random_latents = np.zeros((random_imgs.shape[0], 6))\n",
        "    for i in range(6):\n",
        "        random_latents[:, i] = \\\n",
        "            latents_possible_values[latents_names[i]][random_latent_ids[:, i]]\n",
        "\n",
        "    assert np.all(random_latents[:, 0] == 1)\n",
        "    assert np.min(random_latents[:, 1]) == 1\n",
        "    assert np.max(random_latents[:, 1]) == 3\n",
        "\n",
        "    random_latents = random_latents[:, 1:]\n",
        "    random_latents[:, 0] -= 1.0\n",
        "\n",
        "    metric_data_img_with_latent = {\n",
        "        \"img\": random_imgs,\n",
        "        \"latent\": random_latents,\n",
        "        \"latent_id\": random_latent_ids[:, 1:],\n",
        "        \"is_continuous\": [False, True, True, True, True]}\n",
        "\n",
        "    metric_data = {\n",
        "        \"groups\": metric_data_groups,\n",
        "        \"img_eval_std\": metric_data_eval_std,\n",
        "        \"img_with_latent\": metric_data_img_with_latent}\n",
        "\n",
        "    return imgs, metric_data, latent_values, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlxQXB32srdv"
      },
      "outputs": [],
      "source": [
        "class Metric(object):\n",
        "    # def __init__(self, sess):\n",
        "    #     self.sess = sess\n",
        "    #     self.model = None\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def build(self):\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SAPMetric(Metric):\n",
        "    \"\"\" Impementation of the metric in: \n",
        "        VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED \n",
        "        OBSERVATIONS\n",
        "        Part of the code is adapted from:\n",
        "        https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/sap_score.py\n",
        "    \"\"\"\n",
        "    def __init__(self, metric_data, *args, **kwargs):\n",
        "        super(SAPMetric, self).__init__(*args, **kwargs)\n",
        "        self.metric_data = metric_data\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        data_inference = self.model.inference_from(\n",
        "            self.metric_data[\"img_with_latent\"][\"img\"])\n",
        "        data_gt_latents = self.metric_data[\"img_with_latent\"][\"latent\"]\n",
        "        factor_is_continuous = \\\n",
        "            self.metric_data[\"img_with_latent\"][\"is_continuous\"]\n",
        "\n",
        "        num_latents = data_inference.shape[1]\n",
        "        num_factors = len(factor_is_continuous)\n",
        "\n",
        "        score_matrix = np.zeros([num_latents, num_factors])\n",
        "        for i in range(num_latents):\n",
        "            for j in range(num_factors):\n",
        "                inference_values = data_inference[:, i]\n",
        "                gt_values = data_gt_latents[:, j]\n",
        "                if factor_is_continuous[j]:\n",
        "                    cov = np.cov(inference_values, gt_values, ddof=1)\n",
        "                    assert np.all(np.asarray(list(cov.shape)) == 2)\n",
        "                    cov_cov = cov[0, 1]**2\n",
        "                    cov_sigmas_1 = cov[0, 0]\n",
        "                    cov_sigmas_2 = cov[1, 1]\n",
        "                    score_matrix[i, j] = cov_cov / cov_sigmas_1 / cov_sigmas_2\n",
        "                else:\n",
        "                    gt_values = gt_values.astype(np.int32)\n",
        "                    classifier = LinearSVC(C=0.01, class_weight=\"balanced\")\n",
        "                    classifier.fit(inference_values[:, np.newaxis], gt_values)\n",
        "                    pred = classifier.predict(inference_values[:, np.newaxis])\n",
        "                    score_matrix[i, j] = np.mean(pred == gt_values)\n",
        "        sorted_score_matrix = np.sort(score_matrix, axis=0)\n",
        "        score = np.mean(sorted_score_matrix[-1, :] - \n",
        "                        sorted_score_matrix[-2, :])\n",
        "\n",
        "        return {\"SAP_metric\": score,\n",
        "                \"SAP_metric_detail\": score_matrix}\n",
        "\n",
        "\n",
        "class MIGMetric(Metric):\n",
        "    \"\"\" Impementation of the metric in: \n",
        "        Isolating Sources of Disentanglement in Variational Autoencoders\n",
        "        Part of the code is adapted from:\n",
        "        https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/mig.py\n",
        "    \"\"\"\n",
        "    def __init__(self, metric_data, *args, **kwargs):\n",
        "        super(MIGMetric, self).__init__(*args, **kwargs)\n",
        "        self.metric_data = metric_data\n",
        "\n",
        "    def discretize(self, data, num_bins=20):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        discretized = np.zeros_like(data)\n",
        "        for i in range(data.shape[1]):\n",
        "            discretized[:, i] = np.digitize(\n",
        "                data[:, i],\n",
        "                np.histogram(data[:, i], num_bins)[1][:-1])\n",
        "        return discretized\n",
        "\n",
        "    def mutual_info(self, data1, data2):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        n1 = data1.shape[1]\n",
        "        n2 = data2.shape[1]\n",
        "        mi = np.zeros([n1, n2])\n",
        "        for i in range(n1):\n",
        "            for j in range(n2):\n",
        "                mi[i, j] = mutual_info_score(\n",
        "                    data2[:, j], data1[:, i])\n",
        "        return mi\n",
        "\n",
        "    def entropy(self, data):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        num_factors = data.shape[1]\n",
        "        entr = np.zeros(num_factors)\n",
        "        for i in range(num_factors):\n",
        "            entr[i] = mutual_info_score(data[:, i], data[:, i])\n",
        "        return entr\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        data_inference = self.model.inference_from(\n",
        "            self.metric_data[\"img_with_latent\"][\"img\"])\n",
        "        data_gt_latents = self.metric_data[\"img_with_latent\"][\"latent_id\"]\n",
        "\n",
        "        data_inference_discrete = self.discretize(data_inference)\n",
        "        mi = self.mutual_info(\n",
        "            data_inference_discrete, data_gt_latents)\n",
        "        entropy = self.entropy(data_gt_latents)\n",
        "        sorted_mi = np.sort(mi, axis=0)[::-1]\n",
        "        mig_score = np.mean(\n",
        "            np.divide(sorted_mi[0, :] - sorted_mi[1, :], entropy))\n",
        "\n",
        "        return {\"MIG_metric\": mig_score,\n",
        "                \"MIG_metric_detail_mi\": mi,\n",
        "                \"MIG_metric_detail_entropy\": entropy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "data, metric_data, latent_values, metadata = load_dSprites(\"\")\n",
        "\n",
        "sapMetric_f = SAPMetric(metric_data)\n",
        "sapMetric_f.set_model(model.encode_image)\n",
        "results[\"SAP\"] = sapMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "migMetric_f = MIGMetric(metric_data)\n",
        "migMetric_f.set_model(model.encode_image)\n",
        "results[\"MIG\"] = migMetric_f.evaluate(-1, -1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpu_task_scheduler.gpu_task import GPUTask\n",
        "\n",
        "\n",
        "class GANTask(GPUTask):\n",
        "    def main(self):\n",
        "        import os\n",
        "        import tensorflow as tf\n",
        "        from gan.load_data import load_dSprites\n",
        "        from gan.latent import UniformLatent, JointLatent\n",
        "        from gan.network import Decoder, InfoGANDiscriminator, \\\n",
        "            CrDiscriminator, MetricRegresser\n",
        "        from gan.infogan_cr import INFOGAN_CR\n",
        "        from gan.metric import FactorVAEMetric, DSpritesInceptionScore, \\\n",
        "            DHSICMetric, \\\n",
        "            BetaVAEMetric, SAPMetric, FStatMetric, MIGMetric, DCIMetric\n",
        "        import pickle\n",
        "\n",
        "        data, metric_data, latent_values, metadata = load_dSprites(\"\")\n",
        "        _, height, width, depth = data.shape\n",
        "\n",
        "        latent_list = []\n",
        "\n",
        "        for i in range(self._config[\"uniform_reg_dim\"]):\n",
        "            latent_list.append(UniformLatent(\n",
        "                in_dim=1, out_dim=1, low=-1.0, high=1.0, q_std=1.0,\n",
        "                apply_reg=True))\n",
        "        if self._config[\"uniform_not_reg_dim\"] > 0:\n",
        "            latent_list.append(UniformLatent(\n",
        "                in_dim=self._config[\"uniform_not_reg_dim\"],\n",
        "                out_dim=self._config[\"uniform_not_reg_dim\"],\n",
        "                low=-1.0, high=1.0, q_std=1.0,\n",
        "                apply_reg=False))\n",
        "        latent = JointLatent(latent_list=latent_list)\n",
        "\n",
        "        # decoder = Decoder(\n",
        "        #     output_width=width, output_height=height, output_depth=depth)\n",
        "        # infoGANDiscriminator = \\\n",
        "        #     InfoGANDiscriminator(\n",
        "        #         output_length=latent.reg_out_dim,\n",
        "        #         q_l_dim=self._config[\"q_l_dim\"])\n",
        "        # crDiscriminator = CrDiscriminator(output_length=latent.num_reg_latent)\n",
        "\n",
        "        # shape_network = MetricRegresser(\n",
        "        #     output_length=3,\n",
        "        #     scope_name=\"dSpritesSampleQualityMetric_shape\")\n",
        "\n",
        "        # checkpoint_dir = os.path.join(self._work_dir, \"checkpoint\")\n",
        "        # if not os.path.exists(checkpoint_dir):\n",
        "        #     os.makedirs(checkpoint_dir)\n",
        "        # sample_dir = os.path.join(self._work_dir, \"sample\")\n",
        "        # if not os.path.exists(sample_dir):\n",
        "        #     os.makedirs(sample_dir)\n",
        "        # time_path = os.path.join(self._work_dir, \"time.txt\")\n",
        "        # metric_path = os.path.join(self._work_dir, \"metric.csv\")\n",
        "\n",
        "        # run_config = tf.ConfigProto()\n",
        "        # run_config.gpu_options.allow_growth = True\n",
        "        # with tf.Session(config=run_config) as sess:\n",
        "        #     factorVAEMetric = FactorVAEMetric(metric_data, sess=sess)\n",
        "        #     dSpritesInceptionScore = DSpritesInceptionScore(\n",
        "        #         sess=sess,\n",
        "        #         do_training=False,\n",
        "        #         data=data,\n",
        "        #         metadata=metadata,\n",
        "        #         latent_values=latent_values,\n",
        "        #         network_path=\"metric_model/DSprites\",\n",
        "        #         shape_network=shape_network,\n",
        "        #         sample_dir=sample_dir)\n",
        "        #     dHSICMetric = DHSICMetric(\n",
        "        #         sess=sess,\n",
        "        #         data=data)\n",
        "        #     metric_callbacks = [factorVAEMetric,\n",
        "        #                         dSpritesInceptionScore,\n",
        "        #                         dHSICMetric]\n",
        "        #     gan = INFOGAN_CR(\n",
        "        #         sess=sess,\n",
        "        #         checkpoint_dir=checkpoint_dir,\n",
        "        #         sample_dir=sample_dir,\n",
        "        #         time_path=time_path,\n",
        "        #         epoch=self._config[\"epoch\"],\n",
        "        #         batch_size=self._config[\"batch_size\"],\n",
        "        #         data=data,\n",
        "        #         vis_freq=self._config[\"vis_freq\"],\n",
        "        #         vis_num_sample=self._config[\"vis_num_sample\"],\n",
        "        #         vis_num_rep=self._config[\"vis_num_rep\"],\n",
        "        #         latent=latent,\n",
        "        #         decoder=decoder,\n",
        "        #         infoGANDiscriminator=infoGANDiscriminator,\n",
        "        #         crDiscriminator=crDiscriminator,\n",
        "        #         gap_start=self._config[\"gap_start\"],\n",
        "        #         gap_decrease_times=self._config[\"gap_decrease_times\"],\n",
        "        #         gap_decrease=self._config[\"gap_decrease\"],\n",
        "        #         gap_decrease_batch=self._config[\"gap_decrease_batch\"],\n",
        "        #         cr_coe_start=self._config[\"cr_coe_start\"],\n",
        "        #         cr_coe_increase_times=self._config[\"cr_coe_increase_times\"],\n",
        "        #         cr_coe_increase=self._config[\"cr_coe_increase\"],\n",
        "        #         cr_coe_increase_batch=self._config[\"cr_coe_increase_batch\"],\n",
        "        #         info_coe_de=self._config[\"info_coe_de\"],\n",
        "        #         info_coe_infod=self._config[\"info_coe_infod\"],\n",
        "        #         metric_callbacks=metric_callbacks,\n",
        "        #         metric_freq=self._config[\"metric_freq\"],\n",
        "        #         metric_path=metric_path,\n",
        "        #         output_reverse=self._config[\"output_reverse\"],\n",
        "        #         de_lr=self._config[\"de_lr\"],\n",
        "        #         infod_lr=self._config[\"infod_lr\"],\n",
        "        #         crd_lr=self._config[\"crd_lr\"],\n",
        "        #         summary_freq=self._config[\"summary_freq\"])\n",
        "        #     gan.build()\n",
        "        #     gan.load()\n",
        "\n",
        "        #     results = {}\n",
        "\n",
        "        #     factorVAEMetric_f = FactorVAEMetric(metric_data, sess=sess)\n",
        "        #     factorVAEMetric_f.set_model(gan)\n",
        "        #     results[\"FactorVAE\"] = factorVAEMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "        #     betaVAEMetric_f = BetaVAEMetric(metric_data, sess=sess)\n",
        "        #     betaVAEMetric_f.set_model(gan)\n",
        "        #     results[\"betaVAE\"] = betaVAEMetric_f.evaluate(-1, -1, -1)\n",
        "            \n",
        "            sapMetric_f = SAPMetric(metric_data, sess=sess)\n",
        "            sapMetric_f.set_model(gan)\n",
        "            results[\"SAP\"] = sapMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # fStatMetric_f = FStatMetric(metric_data, sess=sess)\n",
        "            # fStatMetric_f.set_model(gan)\n",
        "            # results[\"FStat\"] = fStatMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            migMetric_f = MIGMetric(metric_data, sess=sess)\n",
        "            migMetric_f.set_model(gan)\n",
        "            results[\"MIG\"] = migMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # for regressor in [\"Lasso\", \"LassoCV\", \"RandomForest\", \"RandomForestIBGAN\", \"RandomForestCV\"]:\n",
        "            #     dciVAEMetric_f = DCIMetric(metric_data, sess=sess, regressor=regressor)\n",
        "            #     dciVAEMetric_f.set_model(gan)\n",
        "            #     results[\"DCI_{}\".format(regressor)] = dciVAEMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # with open(os.path.join(self._work_dir, \"final_metrics.pkl\"), \"wb\") as f:\n",
        "            #     pickle.dump(results, f)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('python38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "96008fafd015bf2e68d95ff2fef5b938b6d1bd2897fe5c2aa0af4ab94e8af56e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
