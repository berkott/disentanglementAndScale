{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WC0tPemnVwf"
      },
      "source": [
        "# Basic CLIP Example\n",
        "A lot of this code is copied from my repo: https://github.com/berkott/cvInterp\n",
        "\n",
        "Also, upload bunny.zip from the paper\n",
        "\n",
        "CLIP repo + download instructions: https://github.com/openai/CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TPL9Kx4TnVwk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "# from absl import logging\n",
        "import numpy as np\n",
        "# from six.moves import range\n",
        "# from sklearn import svm\n",
        "# import gin.tf\n",
        "import os\n",
        "import random\n",
        "import scipy.stats\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import mutual_info_score, roc_auc_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# from sklearn.ensemble.forest import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#TODO:\n",
        "\n",
        "#Get Bunny images\n",
        "#Get pipieline\n",
        "#Test SAP\n",
        "#TEst NIG\n",
        "\n",
        "#Get Geomancer\n",
        "#G\n",
        "\n",
        "\n",
        "#Do different VAE sizes (find different B-VAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_GXjQ2_nVwm"
      },
      "outputs": [],
      "source": [
        "clip.clip._MODELS = {\n",
        "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
        "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
        "    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "}\n",
        "\n",
        "MODELS_TO_USE = [ \"RN50\", \"RN101\", \"RN50x4\"]\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBH_w3mlnVwn"
      },
      "source": [
        "## Model Param Details\n",
        "I think we should just start by running the experiments on RN50, RN101, and RN50x4. These are 3 resnet architectures that are basically just scaled up to different sizes. Then if we have extra time, we cna test on everything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "h0zV4XDTnVwo"
      },
      "outputs": [],
      "source": [
        "# Get model param details\n",
        "# for name in clip.clip._MODELS_TO_USE:\n",
        "#     m, _ = clip.load(name, device=device, jit=False)\n",
        "#     print(name)\n",
        "#     print(f\"Num Params: {sum(p.numel() for p in m.parameters())}\")\n",
        "#     print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97-VUhaWnVwp"
      },
      "outputs": [],
      "source": [
        "image_paths = os.listdir('./drive/MyDrive/images')\n",
        "images = []\n",
        "for path in image_paths[:1000]:\n",
        "  images.append(preprocess(Image.open(\"./drive/MyDrive/images/\" + path)).unsqueeze(0).to(device)) # TODO: Specify correct image here!!\n",
        "\n",
        "# T.ToPILImage()(image[0]).show()\n",
        "\n",
        "\n",
        "# text = clip.tokenize([\"bunny\"]).to(device)\n",
        "# image_encodings = []\n",
        "# for image in images:\n",
        "#   with torch.no_grad():\n",
        "#       image_encodings.append(model.encode_image(image))\n",
        "#       text_features = model.encode_text(text)\n",
        "\n",
        "#       # THIS IMAGE FEATURES VECTOR IS THE LATENT REPRESENTATION WE WANT!!!\n",
        "#       # print(image_features.shape)\n",
        "#       # print(image_features)\n",
        "      \n",
        "#       logits_per_image, logits_per_text = model(image, text)\n",
        "#       probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "#   print(\"Label probs:\", probs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Xgz1CkCBqg"
      },
      "source": [
        "# Mine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dSprites():\n",
        "    # part of the code is from:\n",
        "    # https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_reloading_example.ipynb\n",
        "    dataset_zip = np.load(\"dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\", allow_pickle=True)\n",
        "    imgs = dataset_zip['imgs']\n",
        "    latent_values = dataset_zip['latents_values']\n",
        "    #latents_classes = dataset_zip['latents_classes']\n",
        "\n",
        "    metadata = {\n",
        "        \"lat_names\": ('shape', 'scale', 'orientation', 'posX', 'posY'),\n",
        "        \"lat_sizes\": np.array([3, 6, 40, 32, 32]),\n",
        "        \"lat_values\": {'posX': np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
        "                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,\n",
        "                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,\n",
        "                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,\n",
        "                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,\n",
        "                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
        "                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,\n",
        "                                    0.93548387, 0.96774194, 1.]),\n",
        "                  'posY': np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
        "                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,\n",
        "                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,\n",
        "                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,\n",
        "                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,\n",
        "                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
        "                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,\n",
        "                                    0.93548387, 0.96774194, 1.]),\n",
        "                  'scale': np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.]),\n",
        "                  'orientation': np.array([0., 0.16110732, 0.32221463, 0.48332195,\n",
        "                                           0.64442926, 0.80553658, 0.96664389, 1.12775121,\n",
        "                                           1.28885852, 1.44996584, 1.61107316, 1.77218047,\n",
        "                                           1.93328779, 2.0943951, 2.25550242, 2.41660973,\n",
        "                                           2.57771705, 2.73882436, 2.89993168, 3.061039,\n",
        "                                           3.22214631, 3.38325363, 3.54436094, 3.70546826,\n",
        "                                           3.86657557, 4.02768289, 4.1887902, 4.34989752,\n",
        "                                           4.51100484, 4.67211215, 4.83321947, 4.99432678,\n",
        "                                           5.1554341, 5.31654141, 5.47764873, 5.63875604,\n",
        "                                           5.79986336, 5.96097068, 6.12207799, 6.28318531]),\n",
        "                  'shape': np.array([1., 2., 3.]),\n",
        "                  'color': np.array([1.])}\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "    # metadata = dataset_zip['metadata'][()]\n",
        "    # metadata = \"\"\n",
        "\n",
        "    imgs = imgs.reshape(737280, 64, 64, 1).astype(np.float)  # 0 ~ 1\n",
        "\n",
        "    latents_names = metadata[\"latents_names\"]\n",
        "    latents_sizes = metadata[\"latents_sizes\"]\n",
        "    latents_possible_values = metadata[\"latents_possible_values\"]\n",
        "    latents_bases = np.concatenate(\n",
        "        (latents_sizes[::-1].cumprod()[::-1][1:], np.array([1, ])))\n",
        "\n",
        "    def latent_to_index(latents):\n",
        "        return np.dot(latents, latents_bases).astype(int)\n",
        "\n",
        "    def sample_latent(size=1):\n",
        "        samples = np.zeros((size, latents_sizes.size))\n",
        "        for lat_i, lat_size in enumerate(latents_sizes):\n",
        "            samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
        "        return samples\n",
        "\n",
        "    metric_data_groups = []\n",
        "    L = 100\n",
        "    M = 500\n",
        "\n",
        "    for i in range(M):\n",
        "        fixed_latent_id = i % 5 + 1\n",
        "        latents_sampled = sample_latent(size=L)\n",
        "        latents_sampled[:, fixed_latent_id] = \\\n",
        "            np.random.randint(latents_sizes[fixed_latent_id], size=1)\n",
        "        # print(latents_sampled[0:10])\n",
        "        indices_sampled = latent_to_index(latents_sampled)\n",
        "        imgs_sampled = imgs[indices_sampled]\n",
        "        metric_data_groups.append(\n",
        "            {\"img\": imgs_sampled,\n",
        "             \"label\": fixed_latent_id - 1})\n",
        "\n",
        "    selected_ids = np.random.permutation(range(imgs.shape[0]))\n",
        "    selected_ids = selected_ids[0: imgs.shape[0] / 10]\n",
        "    metric_data_eval_std = imgs[selected_ids]\n",
        "\n",
        "    random_latent_ids = sample_latent(size=imgs.shape[0] / 10)\n",
        "    random_latent_ids = random_latent_ids.astype(np.int32)\n",
        "    random_ids = latent_to_index(random_latent_ids)\n",
        "    assert random_latent_ids.shape == (imgs.shape[0] / 10, 6)\n",
        "    random_imgs = imgs[random_ids]\n",
        "\n",
        "    random_latents = np.zeros((random_imgs.shape[0], 6))\n",
        "    for i in range(6):\n",
        "        random_latents[:, i] = \\\n",
        "            latents_possible_values[latents_names[i]][random_latent_ids[:, i]]\n",
        "\n",
        "    assert np.all(random_latents[:, 0] == 1)\n",
        "    assert np.min(random_latents[:, 1]) == 1\n",
        "    assert np.max(random_latents[:, 1]) == 3\n",
        "\n",
        "    random_latents = random_latents[:, 1:]\n",
        "    random_latents[:, 0] -= 1.0\n",
        "\n",
        "    metric_data_img_with_latent = {\n",
        "        \"img\": random_imgs,\n",
        "        \"latent\": random_latents,\n",
        "        \"latent_id\": random_latent_ids[:, 1:],\n",
        "        \"is_continuous\": [False, True, True, True, True]}\n",
        "\n",
        "    metric_data = {\n",
        "        \"groups\": metric_data_groups,\n",
        "        \"img_eval_std\": metric_data_eval_std,\n",
        "        \"img_with_latent\": metric_data_img_with_latent}\n",
        "\n",
        "    return imgs, metric_data, latent_values, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HlxQXB32srdv"
      },
      "outputs": [],
      "source": [
        "class Metric(object):\n",
        "    # def __init__(self, sess):\n",
        "    #     self.sess = sess\n",
        "    #     self.model = None\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def build(self):\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SAPMetric(Metric):\n",
        "    \"\"\" Impementation of the metric in: \n",
        "        VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED \n",
        "        OBSERVATIONS\n",
        "        Part of the code is adapted from:\n",
        "        https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/sap_score.py\n",
        "    \"\"\"\n",
        "    def __init__(self, metric_data, *args, **kwargs):\n",
        "        super(SAPMetric, self).__init__(*args, **kwargs)\n",
        "        self.metric_data = metric_data\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        data_inference = self.model.inference_from(\n",
        "            self.metric_data[\"img_with_latent\"][\"img\"])\n",
        "        data_gt_latents = self.metric_data[\"img_with_latent\"][\"latent\"]\n",
        "        factor_is_continuous = \\\n",
        "            self.metric_data[\"img_with_latent\"][\"is_continuous\"]\n",
        "\n",
        "        num_latents = data_inference.shape[1]\n",
        "        num_factors = len(factor_is_continuous)\n",
        "\n",
        "        score_matrix = np.zeros([num_latents, num_factors])\n",
        "        for i in range(num_latents):\n",
        "            for j in range(num_factors):\n",
        "                inference_values = data_inference[:, i]\n",
        "                gt_values = data_gt_latents[:, j]\n",
        "                if factor_is_continuous[j]:\n",
        "                    cov = np.cov(inference_values, gt_values, ddof=1)\n",
        "                    assert np.all(np.asarray(list(cov.shape)) == 2)\n",
        "                    cov_cov = cov[0, 1]**2\n",
        "                    cov_sigmas_1 = cov[0, 0]\n",
        "                    cov_sigmas_2 = cov[1, 1]\n",
        "                    score_matrix[i, j] = cov_cov / cov_sigmas_1 / cov_sigmas_2\n",
        "                else:\n",
        "                    gt_values = gt_values.astype(np.int32)\n",
        "                    classifier = LinearSVC(C=0.01, class_weight=\"balanced\")\n",
        "                    classifier.fit(inference_values[:, np.newaxis], gt_values)\n",
        "                    pred = classifier.predict(inference_values[:, np.newaxis])\n",
        "                    score_matrix[i, j] = np.mean(pred == gt_values)\n",
        "        sorted_score_matrix = np.sort(score_matrix, axis=0)\n",
        "        score = np.mean(sorted_score_matrix[-1, :] - \n",
        "                        sorted_score_matrix[-2, :])\n",
        "\n",
        "        return {\"SAP_metric\": score,\n",
        "                \"SAP_metric_detail\": score_matrix}\n",
        "\n",
        "\n",
        "class MIGMetric(Metric):\n",
        "    \"\"\" Impementation of the metric in: \n",
        "        Isolating Sources of Disentanglement in Variational Autoencoders\n",
        "        Part of the code is adapted from:\n",
        "        https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/mig.py\n",
        "    \"\"\"\n",
        "    def __init__(self, metric_data, *args, **kwargs):\n",
        "        super(MIGMetric, self).__init__(*args, **kwargs)\n",
        "        self.metric_data = metric_data\n",
        "\n",
        "    def discretize(self, data, num_bins=20):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        discretized = np.zeros_like(data)\n",
        "        for i in range(data.shape[1]):\n",
        "            discretized[:, i] = np.digitize(\n",
        "                data[:, i],\n",
        "                np.histogram(data[:, i], num_bins)[1][:-1])\n",
        "        return discretized\n",
        "\n",
        "    def mutual_info(self, data1, data2):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        n1 = data1.shape[1]\n",
        "        n2 = data2.shape[1]\n",
        "        mi = np.zeros([n1, n2])\n",
        "        for i in range(n1):\n",
        "            for j in range(n2):\n",
        "                mi[i, j] = mutual_info_score(\n",
        "                    data2[:, j], data1[:, i])\n",
        "        return mi\n",
        "\n",
        "    def entropy(self, data):\n",
        "        \"\"\" Adapted from:\n",
        "            https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/evaluation/metrics/utils.py\n",
        "        \"\"\"\n",
        "        num_factors = data.shape[1]\n",
        "        entr = np.zeros(num_factors)\n",
        "        for i in range(num_factors):\n",
        "            entr[i] = mutual_info_score(data[:, i], data[:, i])\n",
        "        return entr\n",
        "\n",
        "    def evaluate(self, epoch_id, batch_id, global_id):\n",
        "        data_inference = self.model.inference_from(\n",
        "            self.metric_data[\"img_with_latent\"][\"img\"])\n",
        "        data_gt_latents = self.metric_data[\"img_with_latent\"][\"latent_id\"]\n",
        "\n",
        "        data_inference_discrete = self.discretize(data_inference)\n",
        "        mi = self.mutual_info(\n",
        "            data_inference_discrete, data_gt_latents)\n",
        "        entropy = self.entropy(data_gt_latents)\n",
        "        sorted_mi = np.sort(mi, axis=0)[::-1]\n",
        "        mig_score = np.mean(\n",
        "            np.divide(sorted_mi[0, :] - sorted_mi[1, :], entropy))\n",
        "\n",
        "        return {\"MIG_metric\": mig_score,\n",
        "                \"MIG_metric_detail_mi\": mi,\n",
        "                \"MIG_metric_detail_entropy\": entropy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_65079/3545523731.py:48: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  imgs = imgs.reshape(737280, 64, 64, 1).astype(np.float)  # 0 ~ 1\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "data, metric_data, latent_values, metadata = load_dSprites()\n",
        "\n",
        "sapMetric_f = SAPMetric(metric_data)\n",
        "sapMetric_f.set_model(model.encode_image)\n",
        "results[\"SAP\"] = sapMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "migMetric_f = MIGMetric(metric_data)\n",
        "migMetric_f.set_model(model.encode_image)\n",
        "results[\"MIG\"] = migMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpu_task_scheduler.gpu_task import GPUTask\n",
        "\n",
        "\n",
        "class GANTask(GPUTask):\n",
        "    def main(self):\n",
        "        import os\n",
        "        import tensorflow as tf\n",
        "        from gan.load_data import load_dSprites\n",
        "        from gan.latent import UniformLatent, JointLatent\n",
        "        from gan.network import Decoder, InfoGANDiscriminator, \\\n",
        "            CrDiscriminator, MetricRegresser\n",
        "        from gan.infogan_cr import INFOGAN_CR\n",
        "        from gan.metric import FactorVAEMetric, DSpritesInceptionScore, \\\n",
        "            DHSICMetric, \\\n",
        "            BetaVAEMetric, SAPMetric, FStatMetric, MIGMetric, DCIMetric\n",
        "        import pickle\n",
        "\n",
        "        data, metric_data, latent_values, metadata = load_dSprites(\"\")\n",
        "        _, height, width, depth = data.shape\n",
        "\n",
        "        latent_list = []\n",
        "\n",
        "        for i in range(self._config[\"uniform_reg_dim\"]):\n",
        "            latent_list.append(UniformLatent(\n",
        "                in_dim=1, out_dim=1, low=-1.0, high=1.0, q_std=1.0,\n",
        "                apply_reg=True))\n",
        "        if self._config[\"uniform_not_reg_dim\"] > 0:\n",
        "            latent_list.append(UniformLatent(\n",
        "                in_dim=self._config[\"uniform_not_reg_dim\"],\n",
        "                out_dim=self._config[\"uniform_not_reg_dim\"],\n",
        "                low=-1.0, high=1.0, q_std=1.0,\n",
        "                apply_reg=False))\n",
        "        latent = JointLatent(latent_list=latent_list)\n",
        "\n",
        "        # decoder = Decoder(\n",
        "        #     output_width=width, output_height=height, output_depth=depth)\n",
        "        # infoGANDiscriminator = \\\n",
        "        #     InfoGANDiscriminator(\n",
        "        #         output_length=latent.reg_out_dim,\n",
        "        #         q_l_dim=self._config[\"q_l_dim\"])\n",
        "        # crDiscriminator = CrDiscriminator(output_length=latent.num_reg_latent)\n",
        "\n",
        "        # shape_network = MetricRegresser(\n",
        "        #     output_length=3,\n",
        "        #     scope_name=\"dSpritesSampleQualityMetric_shape\")\n",
        "\n",
        "        # checkpoint_dir = os.path.join(self._work_dir, \"checkpoint\")\n",
        "        # if not os.path.exists(checkpoint_dir):\n",
        "        #     os.makedirs(checkpoint_dir)\n",
        "        # sample_dir = os.path.join(self._work_dir, \"sample\")\n",
        "        # if not os.path.exists(sample_dir):\n",
        "        #     os.makedirs(sample_dir)\n",
        "        # time_path = os.path.join(self._work_dir, \"time.txt\")\n",
        "        # metric_path = os.path.join(self._work_dir, \"metric.csv\")\n",
        "\n",
        "        # run_config = tf.ConfigProto()\n",
        "        # run_config.gpu_options.allow_growth = True\n",
        "        # with tf.Session(config=run_config) as sess:\n",
        "        #     factorVAEMetric = FactorVAEMetric(metric_data, sess=sess)\n",
        "        #     dSpritesInceptionScore = DSpritesInceptionScore(\n",
        "        #         sess=sess,\n",
        "        #         do_training=False,\n",
        "        #         data=data,\n",
        "        #         metadata=metadata,\n",
        "        #         latent_values=latent_values,\n",
        "        #         network_path=\"metric_model/DSprites\",\n",
        "        #         shape_network=shape_network,\n",
        "        #         sample_dir=sample_dir)\n",
        "        #     dHSICMetric = DHSICMetric(\n",
        "        #         sess=sess,\n",
        "        #         data=data)\n",
        "        #     metric_callbacks = [factorVAEMetric,\n",
        "        #                         dSpritesInceptionScore,\n",
        "        #                         dHSICMetric]\n",
        "        #     gan = INFOGAN_CR(\n",
        "        #         sess=sess,\n",
        "        #         checkpoint_dir=checkpoint_dir,\n",
        "        #         sample_dir=sample_dir,\n",
        "        #         time_path=time_path,\n",
        "        #         epoch=self._config[\"epoch\"],\n",
        "        #         batch_size=self._config[\"batch_size\"],\n",
        "        #         data=data,\n",
        "        #         vis_freq=self._config[\"vis_freq\"],\n",
        "        #         vis_num_sample=self._config[\"vis_num_sample\"],\n",
        "        #         vis_num_rep=self._config[\"vis_num_rep\"],\n",
        "        #         latent=latent,\n",
        "        #         decoder=decoder,\n",
        "        #         infoGANDiscriminator=infoGANDiscriminator,\n",
        "        #         crDiscriminator=crDiscriminator,\n",
        "        #         gap_start=self._config[\"gap_start\"],\n",
        "        #         gap_decrease_times=self._config[\"gap_decrease_times\"],\n",
        "        #         gap_decrease=self._config[\"gap_decrease\"],\n",
        "        #         gap_decrease_batch=self._config[\"gap_decrease_batch\"],\n",
        "        #         cr_coe_start=self._config[\"cr_coe_start\"],\n",
        "        #         cr_coe_increase_times=self._config[\"cr_coe_increase_times\"],\n",
        "        #         cr_coe_increase=self._config[\"cr_coe_increase\"],\n",
        "        #         cr_coe_increase_batch=self._config[\"cr_coe_increase_batch\"],\n",
        "        #         info_coe_de=self._config[\"info_coe_de\"],\n",
        "        #         info_coe_infod=self._config[\"info_coe_infod\"],\n",
        "        #         metric_callbacks=metric_callbacks,\n",
        "        #         metric_freq=self._config[\"metric_freq\"],\n",
        "        #         metric_path=metric_path,\n",
        "        #         output_reverse=self._config[\"output_reverse\"],\n",
        "        #         de_lr=self._config[\"de_lr\"],\n",
        "        #         infod_lr=self._config[\"infod_lr\"],\n",
        "        #         crd_lr=self._config[\"crd_lr\"],\n",
        "        #         summary_freq=self._config[\"summary_freq\"])\n",
        "        #     gan.build()\n",
        "        #     gan.load()\n",
        "\n",
        "        #     results = {}\n",
        "\n",
        "        #     factorVAEMetric_f = FactorVAEMetric(metric_data, sess=sess)\n",
        "        #     factorVAEMetric_f.set_model(gan)\n",
        "        #     results[\"FactorVAE\"] = factorVAEMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "        #     betaVAEMetric_f = BetaVAEMetric(metric_data, sess=sess)\n",
        "        #     betaVAEMetric_f.set_model(gan)\n",
        "        #     results[\"betaVAE\"] = betaVAEMetric_f.evaluate(-1, -1, -1)\n",
        "            \n",
        "            sapMetric_f = SAPMetric(metric_data, sess=sess)\n",
        "            sapMetric_f.set_model(gan)\n",
        "            results[\"SAP\"] = sapMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # fStatMetric_f = FStatMetric(metric_data, sess=sess)\n",
        "            # fStatMetric_f.set_model(gan)\n",
        "            # results[\"FStat\"] = fStatMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            migMetric_f = MIGMetric(metric_data, sess=sess)\n",
        "            migMetric_f.set_model(gan)\n",
        "            results[\"MIG\"] = migMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # for regressor in [\"Lasso\", \"LassoCV\", \"RandomForest\", \"RandomForestIBGAN\", \"RandomForestCV\"]:\n",
        "            #     dciVAEMetric_f = DCIMetric(metric_data, sess=sess, regressor=regressor)\n",
        "            #     dciVAEMetric_f.set_model(gan)\n",
        "            #     results[\"DCI_{}\".format(regressor)] = dciVAEMetric_f.evaluate(-1, -1, -1)\n",
        "\n",
        "            # with open(os.path.join(self._work_dir, \"final_metrics.pkl\"), \"wb\") as f:\n",
        "            #     pickle.dump(results, f)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8f6f1c9237a73d014ef6869628adf560dda6003e6cef2a667f3277470f6c79f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
